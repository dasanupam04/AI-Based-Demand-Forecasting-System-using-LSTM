"""
=========================================================
src/model_comparison.py
=========================================================

PROJECT STAGE:
Model Benchmarking & Performance Evaluation

---------------------------------------------------------
üéØ PURPOSE OF THIS FILE
---------------------------------------------------------
This module compares forecasting model performance 
to determine the best predictive model.

Instead of blindly using LSTM, this module evaluates:

‚Ä¢ LSTM performance
‚Ä¢ ARIMA performance
‚Ä¢ (Previously RF, if implemented)

It selects the best model based on objective metrics.

---------------------------------------------------------
üß† WHAT PROBLEM THIS FILE SOLVES
---------------------------------------------------------
Using a single model without comparison is risky.

We must answer:

‚Ä¢ Is LSTM actually better?
‚Ä¢ Does ARIMA perform similarly?
‚Ä¢ Which model generalizes better?

This module provides scientific validation.

---------------------------------------------------------
‚öôÔ∏è WHAT HAPPENS INSIDE THIS FILE
---------------------------------------------------------

1Ô∏è‚É£ Train / Use Pretrained LSTM

Uses sequence-based input.

2Ô∏è‚É£ Train / Use ARIMA Model

Statistical time-series forecasting approach.

3Ô∏è‚É£ Generate Predictions

Both models predict on the same test data.

4Ô∏è‚É£ Calculate Evaluation Metrics

Common metrics:

‚Ä¢ MAE (Mean Absolute Error)
‚Ä¢ RMSE (Root Mean Squared Error)

5Ô∏è‚É£ Compare Results

Model with lowest error selected as best.

Returns:

‚Ä¢ Model performance table
‚Ä¢ Best model name

---------------------------------------------------------
üìä WHY THIS FILE IS IMPORTANT
---------------------------------------------------------
This module:

‚Ä¢ Prevents overfitting bias
‚Ä¢ Validates deep learning usefulness
‚Ä¢ Strengthens academic rigor
‚Ä¢ Improves production reliability

Recruiters love model comparison.

---------------------------------------------------------
üîó HOW IT CONNECTS TO THE PROJECT
---------------------------------------------------------
Used in:

‚Üí app.py (Model Comparison Section)
‚Üí recommendation_engine.py (optional weighting)
‚Üí future retraining logic

Feeds into:

Streamlit dashboard model comparison panel.

---------------------------------------------------------
üèó ARCHITECTURAL POSITION
---------------------------------------------------------
Layer 1: Data
Layer 2: Preprocessing
Layer 3: Forecasting Models
Layer 4: Model Comparison Layer   ‚Üê This file
Layer 5: Strategic Intelligence
Layer 6: Dashboard

---------------------------------------------------------

"I implemented a model comparison module to benchmark LSTM 
against ARIMA using MAE and RMSE metrics, ensuring that the 
chosen model delivers measurable performance improvement."

---------------------------------------------------------
üß† KEY LEARNING
---------------------------------------------------------
Complex models must justify their complexity.

Benchmarking ensures engineering decisions 
are data-driven, not assumption-driven.
=========================================================
"""
